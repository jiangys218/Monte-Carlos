---
title: "Stats 102C, Homework 1"
output: html_document
author: Yunshuang Jiang
header-includes:
- \usepackage{amsmath,amssymb,amsthm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Homework Questions, copyright Miles Chen. Do not post or distribute without permission.

## Reading and Viewing:

- Introducing Monte Carlo Methods with R: Section 2.1, and Section 2.3
- Kolmogorov-Smirnov Test on Youtube: <https://www.youtube.com/watch?v=ZO2RmSkXK3c> (This video covers the two-sample test, but we will conduct a one-sample test against a reference distribution)

## Problem 1 - Estimate pi (poorly)

A "fun" Monte Carlo Exercise ... Get a bad estimate of pi by using random uniform numbers.

In this first exercise, we can see how a simple source of randomness (in our case, R's `runif()` function) can be used to estimate tough quantities.

We will find an estimate of pi by estimating the ratio between the area of a circle and its encompassing square.

```{r}
s <- seq(-1,1, by = 0.001)
posf <- sqrt(1-s^2)
plot(s, posf, type = "l", asp = 1, ylim = c(-1,1))
lines(s, -1*posf)
segments(-1,-1,-1,1)
segments(-1,-1,1,-1)
segments(1,1,-1,1)
segments(1,1,1,-1)
```

To calculate the area of the circle analytically, we would need to integrate the function drawing the upper semi-circle and then multiply that by 2. This process requires the use of trig substitutions, and while doable, can illustrate a time where the analytic solution is not easy.

$$Area = 2 \times \int_{-1}^1 \sqrt{1 - x^2} dx$$

For the Monte-Carlo approach, we will use `runif(n, min = -1, max=1)` to generate a bunch of random pairs of x and y coordinates. We will see how many of those random uniform points fall within the circle. This is easy - just see if $x^2 + y^2 \le 1$. The total area of the square is 4. The total area of the circle is pi. Thus, the proportion of coordinates that satisfy the inequality  $x^2 + y^2 \le 1 \approx \pi/4$.

Instructions:

- create a vector x of n random values between -1 and 1. I suggest starting with n = 500
- create a vector y of n random values between -1 and 1. Use the two vectors to make coordinate pairs.
- calculate which of points satisfy the inequality for falling inside the circle.
- Print out your estimate of pi by multiplying the proportion by 4.
- plot each of those (x,y) coordinate pairs. Use pch = 20. Color the points based on whether they fall in the circle or not.

```{r,message=FALSE, warning=FALSE}
library("plotrix")
x <- runif(500, -1, 1)
y <- runif(500, -1, 1)

within = 0
s <- seq(-1,1, by = 0.001)
posf <- sqrt(1-s^2)
plot(s, posf, type = "l", asp = 1, ylim = c(-1,1))
lines(s, -1*posf)
segments(-1,-1,-1,1)
segments(-1,-1,1,-1)
segments(1,1,-1,1)
segments(1,1,1,-1)

for (i in 1:500) {
  if (x[i]^2 + y[i]^2 <= 1) {
    points(x[i], y[i], col="red", pch=20, asp = 1, xlim = c(-2, 2), ylim=c(-1,1))
    within <- within + 1 }
  
  if (x[i]^2 + y[i]^2 > 1)
    points(x[i], y[i], col="blue", pch=20, asp = 1, xlim = c(-2, 2),  ylim=c(-1,1))
}
draw.circle(0,0,1, lty=1,lwd=1)
est_pi <- within/500 * 4
est_pi
```


## Problem 2

Write a function `my_rexp(n, rate)`, that will generate `n` random values drawn from an exponential distribution with lambda = "rate" by using the inverse CDF method. Use `runif()` as your sole source of randomness.

You are not allowed to use any of the functions `dexp()`, `pexp()`, `qexp()`, or `rexp()`. 

Use your function to generate 500 random samples from an exponential distribution with lambda  = 1.

After generating 500 samples, plot the empirical CDF function of your data (see `ecdf`). Add the theoretic CDF of the exponential distribution to the same plot (in a different color). 

Use the Kolmogorov-Smirnov test to compare your generated samples to the theoretic exponential distribution. Be sure to print out the resulting p-value and comment on the sample produced by your function.

```{r,message=FALSE, warning=FALSE}
myx <- c()
my_rexp <- function(n, rate){
 myx <- (-1/rate)*log(runif(n))
 return(myx)
  }

x <- rexp(500, rate = 1)

plot(ecdf(x))
vals <- seq(0.01, max(x), by = 0.01)
lines(vals, pexp(vals, rate = 1), col = "red")
ks.test(x, pexp)
```

Acorrding to the graph, the sample produced by the function is very close to the theoretical distribution. The p-value is large, which means we have no evidence to conclude the sample is not from the exponential distribution with lambda =1. \newline


## Problem 3

Write a function `my_rbinom(n, size, prob)`, that will generate `n` random values drawn from a binomial distribution with size = `size` and probability of success = `prob` by using the inverse CDF method. Use `runif()` as your sole source of randomness.

Do not use any of R's binom functions. Do not use `dbinom`, `pbinom`, `qbinom()`, or `rbinom()`

Use your function `my_rbinom()` to generate 200 values from a binomial distribution with n = 6, and p = 0.4.

After generating 200 samples, make a side-by-side barchart that shows the empirical PMF of your data and the theoretic PMF according to the binomial distribution.

Use a chi-squared goodness-of-fit test to see if the generated values fit the expected probabilities. Be sure to comment on the graph and results of the test.

```{r,message=FALSE, warning=FALSE}
library(dplyr)
my_rbinom <- function(n, size, prob){
  p <- c()
  cumprob <- c()
  for (i in 0:size) {
    p[i] <- choose(size, i)*(prob^i)*((1-prob)^(size-i)) }
  
 cumprob <- cumsum(p)
 num <- runif(n)
 result <- c()
 for (k in 1:n) {
   if (cumprob[1] >= num[k]) {result[k] = 1} else if (
       cumprob[2] >= num[k]) {result[k] = 2} else if (
           cumprob[3] >= num[k]) {result[k] = 3} else if (
               cumprob[4] >= num[k]) {result[k] = 4} else if (
                   cumprob[5] >= num[k]) {result[k] = 5} else if (
                       cumprob[6] >= num[k]) {result[k] = 6} else {
                         result[k] = 0}
 }
 return(result)
}
myresult <- my_rbinom(200, 6, 0.4)

emp <- data.frame(c(0:6), c(sum(unlist(myresult)==0),sum(unlist(myresult)==1), sum(unlist(myresult)==2), sum(unlist(myresult)==3),sum(unlist(myresult)==4), sum(unlist(myresult)==5), sum(unlist(myresult)==6)))

bar_table <- as.vector(table(rbinom(200, 6, 0.4)))
bar_table <- rbind(as.vector(emp[,2]),bar_table)
barplot(bar_table, beside=T, names.arg = c(0:6))

theo = dbinom(0:6, 6, 0.4)
emp[6,2] <- emp[6,2] + emp[7,2]
emp <- emp[1:6,]
theo1 = c(theo[1:5], theo[6]+theo[7])
chisq.test(emp[,2], p = theo1)
```

The graphs shows that the theoretical count and empirical count are very similar. The p-value is large, which means we have no evidence to conclude the sample is not from the binomial distribution with size = 6 and probability = 0.4. \newline


## Problem 4

Let $f(x)$ and $g(x)$ be the target and candidate (proposal) distributions, respectively, in acceptance-rejection sampling. Find the optimal constant M that maximizes the acceptance rates for the following designs.

$f(x) = \frac{1}{2} \sin(x)$ for $0 \le x \le \pi$

$g(x) = \mbox{Unif}(0, \pi)$

#### Answer: M is pi/2

Implement the rejection sampling design, using `runif(n, 0, pi)` as your source of randomness. Generate 500 samples.


```{r,message=FALSE, warning=FALSE}
f <- function(x){0.5* sin(x)}

accept <- c()
total_iter <- 0
while(length(accept) < 500) {
  x <- runif(1, 0, pi)
  num <- runif(1)
   if (num <= f(x)) {
     accept[length(accept)+1] <- x
   }
  total_iter = total_iter+1
}


acceptrate <- length(accept)/total_iter
acceptrate

hist(accept)
plot(density(accept))
```
What is your acceptance rate? `r acceptrate`

Create a histogram of your generated (accepted) sample.

Plot a kernel density of the resulting (accepted) sample.

## Problem 5

Use rejection sampling to generate samples from the normal distribution, by using the folded-normal distribution method discussed in class.

The standard normal distribution has the pdf:

$$f(z) = \frac{1}{\sqrt{2\pi}} \exp{(-z^2/2)} \mbox{,   for } z \in (-\infty, \infty)$$

The target distribution f(x) will be the positive half of the standard normal distribution, which will have PDF:

$$f(x) = 2 \times \frac{1}{\sqrt{2\pi}} \exp{(-x^2/2)}\mbox{,   for } x \ge 0$$

Use an exponential distribution with lambda = 1 as your trial (proposal) distribution.

$$g(x) = e^{-x} \mbox{,   for } x \ge 0$$

Find the optimal constant M that maximizes the acceptance rates for the rejection sampling design.

Implement the rejection sampling design as discussed in class.

- Use `runif` and inverse CDF to get a proposal value $X$ from the exponential distribution.
- Calculate the ratio: $\frac{f(X)}{M \times g(X)}$
- Use `runif` to generate $U$ to decide whether to accept or reject the proposed $X$.
- keep the accepted $X$
- Use `runif` to generate $S$ to decide whether the accepted $X$ will be positive or negative with probably 0.5.

Use the above algorithm to generate a vector of 200 random values from the normal distribution.

Create a histogram of your generated sample.

Create a QQ-norm plot.
```{r,message=FALSE, warning=FALSE}
fx <- function(x){2*(1/(sqrt(2*pi)))*exp(-(x^2)/2)}
gx <- function(x){exp(-x)}

# max x (fx/gx) = max x ((x^2)/2 -x) => x=1
#M = f(M)/g(M) = 2*1/sqrt(2*pi)* exp^ -(1/2 - 1) = 1.31548924696
m = 1.31548924696

func <- function(n){
  iter <- 1
  accept <- c()
  while(length(accept) < 200){
    num <- runif(1)
    my_x <- -log(runif(1))
    if (num <= fx(my_x)/(m*gx(my_x))) {
      s <- runif(1)
      if( s <= 0.5) {
         accept[length(accept)+1] <- -my_x
    }else{
      accept[length(accept)+1] <- my_x
    }
    }
  }
  return(accept)
}
  
gendata <- func(200)

hist(gendata)
qqnorm(gendata)
```

